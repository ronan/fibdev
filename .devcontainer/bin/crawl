#!/bin/bash

# PROD_URL=http://drupal
# DEPTH_TO_CRAWL=3

WGET_OUT=/workspace/outbox/clone

say "🕸️ Crawling $PROD_URL to depth $DEPTH_TO_CRAWL"

mkdir -p $WGET_OUT
  rm -rf $WGET_OUT/*

wget \
      --spider \
      --recursive \
      --force-html \
      --max-redirect=2 \
      --level=$DEPTH_TO_CRAWL \
      --user-agent=backdev/crawlbot \
      --exclude-directories=/sites/default/files \
      --execute robots=off \
      $PROD_URL 2>&1 \
      | grep -B 3 "\[text/html\]" \
      | grep $PROD_URL \
      | awk "/--/{gsub(\"$PROD_URL\", \"\", \$3); print \$3}" \
      | sort \
      | uniq \
      > /workspace/outbox/urls.txt